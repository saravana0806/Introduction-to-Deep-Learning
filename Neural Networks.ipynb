{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55bfc8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning using Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17a6349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "32/32 [==============================] - 3s 17ms/step - loss: 0.6958 - accuracy: 0.4750 - val_loss: 0.6870 - val_accuracy: 0.5350\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6906 - accuracy: 0.5350 - val_loss: 0.6989 - val_accuracy: 0.4750\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6882 - accuracy: 0.5500 - val_loss: 0.6889 - val_accuracy: 0.5400\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6883 - accuracy: 0.5410 - val_loss: 0.6898 - val_accuracy: 0.5350\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6853 - accuracy: 0.5520 - val_loss: 0.6919 - val_accuracy: 0.5350\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6845 - accuracy: 0.5470 - val_loss: 0.6992 - val_accuracy: 0.5050\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6823 - accuracy: 0.5710 - val_loss: 0.6949 - val_accuracy: 0.5150\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6816 - accuracy: 0.5710 - val_loss: 0.6969 - val_accuracy: 0.4950\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6797 - accuracy: 0.5670 - val_loss: 0.6914 - val_accuracy: 0.5450\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6793 - accuracy: 0.5680 - val_loss: 0.6926 - val_accuracy: 0.5400\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5400\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "np.random.seed(42) \n",
    "num_samples = 1000\n",
    "num_features = 10\n",
    "\n",
    "X_train = np.random.rand(num_samples, num_features)  \n",
    "y_train = np.random.randint(0, 2, size=(num_samples, 1)) \n",
    "num_test_samples = 200  \n",
    "X_test = np.random.rand(num_test_samples, num_features)  \n",
    "y_test = np.random.randint(0, 2, size=(num_test_samples, 1)) \n",
    "\n",
    "def create_ann_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_ann_model()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be4782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5989134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567db34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f698cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # for reproducibility\n",
    "num_samples = 1000\n",
    "num_features = 10\n",
    "\n",
    "X_train = np.random.rand(num_samples, num_features)  # Sample features\n",
    "y_train = np.random.randint(0, 2, size=(num_samples, 1))  # Binary labels (0 or 1)\n",
    "\n",
    "num_test_samples = 200  # Number of test samples\n",
    "\n",
    "X_test = np.random.rand(num_test_samples, num_features)  # Sample features for testing\n",
    "y_test = np.random.randint(0, 2, size=(num_test_samples, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "641338bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 3s 27ms/step - loss: 0.7037 - accuracy: 0.4889 - val_loss: 0.7142 - val_accuracy: 0.4700\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.6893 - accuracy: 0.5156 - val_loss: 0.7133 - val_accuracy: 0.4500\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.6901 - accuracy: 0.5256 - val_loss: 0.7096 - val_accuracy: 0.4400\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.6916 - accuracy: 0.5267 - val_loss: 0.7076 - val_accuracy: 0.4400\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.6857 - accuracy: 0.5456 - val_loss: 0.7129 - val_accuracy: 0.4000\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.6872 - accuracy: 0.5478 - val_loss: 0.7129 - val_accuracy: 0.4800\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.6860 - accuracy: 0.5511 - val_loss: 0.7177 - val_accuracy: 0.4100\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 9ms/step - loss: 0.6898 - accuracy: 0.5433 - val_loss: 0.7163 - val_accuracy: 0.4500\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.6862 - accuracy: 0.5411 - val_loss: 0.7187 - val_accuracy: 0.4300\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 7ms/step - loss: 0.6861 - accuracy: 0.5611 - val_loss: 0.7134 - val_accuracy: 0.3800\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6973 - accuracy: 0.5300\n",
      "Test Loss: 0.6972925662994385\n",
      "Test Accuracy: 0.5299999713897705\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    SimpleRNN(64, input_shape=(num_features, 1)), \n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_train_rnn = X_train.reshape(-1, num_features, 1)\n",
    "X_test_rnn = X_test.reshape(-1, num_features, 1)\n",
    "\n",
    "history = model.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_rnn, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24daae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc2e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc87294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a3ee509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 53s 0us/step\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 43s 26ms/step - loss: 1.4697 - accuracy: 0.4687 - val_loss: 1.1826 - val_accuracy: 0.5874\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 1.1119 - accuracy: 0.6119 - val_loss: 1.0231 - val_accuracy: 0.6468\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.9857 - accuracy: 0.6545 - val_loss: 1.0894 - val_accuracy: 0.6158\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 37s 23ms/step - loss: 0.9055 - accuracy: 0.6845 - val_loss: 0.9757 - val_accuracy: 0.6617\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.8493 - accuracy: 0.7060 - val_loss: 0.9593 - val_accuracy: 0.6658\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 39s 25ms/step - loss: 0.7956 - accuracy: 0.7231 - val_loss: 0.9124 - val_accuracy: 0.6889\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.7496 - accuracy: 0.7400 - val_loss: 0.9353 - val_accuracy: 0.6877\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 0.7089 - accuracy: 0.7528 - val_loss: 0.9128 - val_accuracy: 0.6899\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 0.6703 - accuracy: 0.7662 - val_loss: 0.9065 - val_accuracy: 0.6975\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 0.6360 - accuracy: 0.7766 - val_loss: 0.9081 - val_accuracy: 0.6926\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.9081 - accuracy: 0.6926\n",
      "Test Loss: 0.9080723524093628\n",
      "Test Accuracy: 0.6926000118255615\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),  # Convolutional layer with 32 filters, each 3x3, ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
    "    Conv2D(64, (3, 3), activation='relu'),  # Convolutional layer with 64 filters, each 3x3, ReLU activation\n",
    "    MaxPooling2D((2, 2)),  \n",
    "    Flatten(),  # Flatten layer to convert 2D feature maps to 1D feature vectors\n",
    "    Dense(64, activation='relu'),  # Fully connected layer\n",
    "    Dense(10, activation='softmax')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c511a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331d5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6a0d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b414be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "vocab_size = 10000  \n",
    "max_length = 200  \n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),  # Embedding layer\n",
    "    LSTM(64),  # LSTM layer with 64 units\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation function\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=3, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418a87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d97ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9206aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e503e01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Pooling layers are used to reduce the spatial dimensions (width and height) of the input volume, \\nwhile preserving the depth. The main purpose of pooling is to progressively reduce the spatial size \\nof the representation, leading to a reduction in the number of parameters and computation in the \\nnetwork, as well as to control overfitting.\\n\\nMax Pooling & Average Pooling\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Pooling layers are used to reduce the spatial dimensions (width and height) of the input volume, \n",
    "while preserving the depth. The main purpose of pooling is to progressively reduce the spatial size \n",
    "of the representation, leading to a reduction in the number of parameters and computation in the \n",
    "network, as well as to control overfitting.\n",
    "\n",
    "Max Pooling & Average Pooling\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a27809aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7ed13a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dense layers, also known as fully connected layers, connect each neuron in one layer to every \\nneuron in the next layer. They perform a linear operation on the input, followed by a non-linear \\nactivation function'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Dense layers, also known as fully connected layers, connect each neuron in one layer to every \n",
    "neuron in the next layer. They perform a linear operation on the input, followed by a non-linear \n",
    "activation function'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bc7d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ee68f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the term \"compile\" refers to the process of configuring the model for training by specifying \\nthe optimizer, loss function, and metrics to use.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' the term \"compile\" refers to the process of configuring the model for training by specifying \n",
    "the optimizer, loss function, and metrics to use.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "deb6bbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' different optimizers like (adam, SGD, and RMSprop)\\n    different loss functions (binary_crossentropy and mean_squared_error) \\n    different metrics (accuracy, Precision, Recall, and AUC) \\n    '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' different optimizers like (adam, SGD, and RMSprop)\n",
    "    different loss functions (binary_crossentropy and mean_squared_error) \n",
    "    different metrics (accuracy, Precision, Recall, and AUC) \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c54a472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6437c9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Long Short-Term Memory (LSTM) layer is a type of recurrent neural network (RNN) layer that is \\ncapable of learning long-term dependencies in sequential data. It is particularly well-suited for \\nsequence prediction tasks where there is a long time lag between relevant inputs'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''A Long Short-Term Memory (LSTM) layer is a type of recurrent neural network (RNN) layer that is \n",
    "capable of learning long-term dependencies in sequential data. It is particularly well-suited for \n",
    "sequence prediction tasks where there is a long time lag between relevant inputs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096aa46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3128c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An embedding layer in neural networks is a type of layer commonly used in natural language \\nprocessing (NLP) tasks, particularly in tasks involving text data. It is used to map categorical \\nvariables, such as words or characters, to dense vectors of fixed size, called embeddings'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''An embedding layer in neural networks is a type of layer commonly used in natural language \n",
    "processing (NLP) tasks, particularly in tasks involving text data. It is used to map categorical \n",
    "variables, such as words or characters, to dense vectors of fixed size, called embeddings'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b6fae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b01bc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Flatten layer in neural networks is used to convert multi-dimensional input data into \\na one-dimensional array.\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The Flatten layer in neural networks is used to convert multi-dimensional input data into \n",
    "a one-dimensional array.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfa34cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid range(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6788c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ee80e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperbolic Tangent (tanh) Range: (-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24c6baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebf93451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectified Linear Unit (ReLU) Range: [0, +∞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9802d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fac8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky ReLU Function Range: (-∞, +∞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05311ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = leaky_relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b5f4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Function Range: (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8257e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    exp_vals = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
    "\n",
    "x = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "y = softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8a15f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the right activation functions\n",
    "\n",
    "# Nature of the Problem: binary, multi-class\n",
    "# Avoiding Vanishing and Exploding Gradients\n",
    "# Interpretability\n",
    "# Neural Network Architectures: LSTMs use tanh activation, CNNs commonly use ReLU in hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df989ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
